Industries are using Hadoop extensively to analyze their data sets. The reason is that Hadoop framework is based on a simple programming model (MapReduce) and it enables a computing solution that is scalable, flexible, fault-tolerant and cost effective. Here, the main concern is to maintain speed in processing large datasets in terms of waiting time between queries and waiting time to run the program.', '', 'Spark was introduced by Apache Software Foundation for speeding up the Hadoop computational computing software process.', '', 'As against a common belief,\xa0Spark is not a modified version of Hadoopand is not, really, dependent on Hadoop because it has its own cluster management. Hadoop is just one of the ways to implement Spark.', '', 'Spark uses Hadoop in two ways â€“ one is\xa0storage\xa0and second is\xa0processing. Since Spark has its own cluster management computation, it uses Hadoop for storage purpose only.